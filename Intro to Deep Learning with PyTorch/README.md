* Github DL-v2-pytorch repo - https://github.com/udacity/deep-learning-v2-pytorch
* Notion Notes - https://www.notion.so/e1250/Intro-to-Deep-Learning-with-PyTorch-f333ad09e5174f46969307296dc3f6f9?pvs=4

# Resources
## Capsule Networks
* You can learn more about [capsules, in this blog post.](https://cezannec.github.io/Capsule_Networks/)
* And experiment with an implementation of a capsule network in PyTorch, [at this github repo.](https://github.com/cezannec/capsule_net_pytorch)
### Supporting Materials
[Dynamic routing between capsules, hinton et al](https://video.udacity-data.com/topher/2018/November/5bfdca4f_dynamic-routing/dynamic-routing.pdf)

## CNNs for Image Classification
* Convolutional Neural Networks (CNNs / ConvNets) - https://cs231n.github.io/convolutional-networks/#conv

## Convolutional Layers in PyTorch
* One by One [ 1 x 1 ] Convolution - https://iamaaditya.github.io/2016/03/one-by-one-convolution/
* we suggest you use cloud platforms such as [AWS](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html), [GCP](https://cloud.google.com/gpu/), and [FloydHub](https://blog.floydhub.com/) to train your networks on a GPU.

## Groundbreaking CNN Architectures
### Optional Resources
* Check out the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) paper!
* Read more about [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) here.
* The [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) paper can be found here.
* Here's the [Keras](https://keras.io/api/applications/) documentation for accessing some famous CNN architectures.
* Read this [detailed treatment](http://neuralnetworksanddeeplearning.com/chap5.html) of the vanishing gradients problem.
* Here's a GitHub [repository](https://github.com/jcjohnson/cnn-benchmarks) containing benchmarks for different CNN architectures.
* Visit the [ImageNet Large Scale Visual Recognition Competition (ILSVRC)](https://image-net.org/challenges/LSVRC/)https://image-net.org/challenges/LSVRC/ website.

## Visualizing CNNs
* Visualizing and Understanding Deep Neural Networks by Matt Zeiler - https://www.youtube.com/watch?v=ghEmQSxT6tw
If you would like to know more about interpreting CNNs and convolutional layers in particular, you are encouraged to check out these resources:
* Here's a [section](https://learn.udacity.com/courses/ud188/lessons/7f6284d3-6c52-49a3-9a10-602f015918af/concepts/b9e7b9cf-70d2-4b0a-ae49-b811c230cd60) from the Stanford's CS231n course on visualizing what CNNs learn.
* Check out this [demonstration](https://experiments.withgoogle.com/what-neural-nets-see) of a cool [OpenFrameworks](https://openframeworks.cc/) app that visualizes CNNs in real-time, from user-supplied video!
* Here's a [demonstration](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s) of another visualization tool for CNNs. If you'd like to learn more about how these visualizations are made, check out this [video](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s).
* Read this[ Keras blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) on visualizing how CNNs see the world. In this post, you can find an accessible introduction to Deep Dreams, along with code for writing your own deep dreams in Keras. When you've read that:
  * Also check out this [music video](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) that makes use of Deep Dreams (look at 3:15-3:40)!
  * Create your own Deep Dreams (without writing any code!) using this [website](https://deepdreamgenerator.com/).
* If you'd like to read more about interpretability of CNNs:
   * Here's an [article](https://blog.openai.com/adversarial-example-research/) that details some dangers from using deep learning models (that are not yet interpretable) in real-world applications.
  * There's a lot of active research in this area. [These authors](https://arxiv.org/abs/1611.03530) recently made a step in the right direction.
## Summary of CNNs
[Deep learning eBook (2016)](https://www.deeplearningbook.org/) authored by Ian Goodfellow, Yoshua Bengio, and Aaron Courville; published by Cambridge: MIT Press.
